One synchronization-related issue with this design is that the communication queues between the mappers and reducers are associated with the mappers. This means that every reducer must cycle through every mapper's queue each time that a new value is available, wasting cpu time.
One possible solution for this issue is to instead associate the queues with the reducers. Each time that a mapper produces a word, it can compute the hash of the word, and put it directly into the appropriate reducer's queue. This approach also requires each item in the queue to be labeled with the mapper that produced it, so that the sorted-reducer logic can still be used. It also requires mappers to produce sentinal "closed objects" in those queues in order to tell the reducers when the mappers are completed.

Another issue is that there is no gaurentee that the reducers will ever complete, since semaphores don't make any guarantee of which task is executed when multiple tasks are waiting for a semaphore which has just been released. For example, reducer 0 could be the only reducer able to take the last item in a mapper's queue, but reducer 1 could perpetually be given the signal semaphore. In practice, there is enough entropy in the operating system for the correct task to be executed eventually, but the delays due to this issue can be augmented to a noticable amount of time by increasing the number of reducers to be vary large.
This issue can be relieved by a variation of the solution to the previous issue, where instead of using any kind of global "signal" semaphore, each reducer blocks on it's own queue's "filled" semaphore.
A way to aliviate this issue without a change in the current logic is to use only just enough reducers to effectively consume the cpu time left by the mapper's time spent blocking on disk operations.
